{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-harrison",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.utils import set_determinism\n",
    "import numpy as np\n",
    "\n",
    "from seg_data import getSegmentationDataset\n",
    "from seg_model import getUNetForSegmentation, getUNETRForSegmentation\n",
    "from transforms_dict import getSegmentationPostProcessingForLabel, getSegmentationPostProcessingForLabelOutput\n",
    "from utils import compute_mean_dice, getReducePlateauScheduler, getAdamOptimizer, loadExistingModel\n",
    "from utils import print_model_output, check_model_name, getDevice\n",
    "\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "from monai.transforms import Activations, AsDiscrete, Activations\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18476ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_gdice = monai.losses.GeneralizedDiceLoss(other_act=nn.Softmax(dim=1), include_background=True)\n",
    "loss_gdicefoc = monai.losses.GeneralizedDiceFocalLoss(other_act=nn.Softmax(dim=1), include_background=True)\n",
    "loss_dicece = monai.losses.DiceCELoss(include_background=False, other_act=nn.Softmax(dim=1))\n",
    "\n",
    "#Metric MUNet\n",
    "def metric_munet(preds, labels):        \n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    preds = preds.detach().cpu().numpy()    \n",
    "    labels[np.where(labels == np.amax(labels, axis=1))] = 1\n",
    "    labels[labels != 1] = 0\n",
    "    dice=2*np.sum(labels*preds,(0,2,3,4))/(np.sum((labels+preds),(0,2,3,4))+1)    \n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454c2da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dataset = \"Femina3\"\n",
    "ft = None\n",
    "ct = None\n",
    "batchsize = 1\n",
    "num_epochs = 500\n",
    "lr = 0.001\n",
    "factor = 0.9\n",
    "patience = 7\n",
    "augment = True\n",
    "N4 = False\n",
    "#Modelname and device\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "#modelname = check_model_name(modelname)\n",
    "#print_model_output(modelname)\n",
    "set_determinism(seed=0)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "device = getDevice()\n",
    "\n",
    "#postprocessing\n",
    "outputs_processing = getSegmentationPostProcessingForLabelOutput()\n",
    "labels_processing = getSegmentationPostProcessingForLabel()\n",
    "\n",
    "#activations\n",
    "softmax = Activations(other=nn.Softmax(dim=1))\n",
    "\n",
    "\n",
    "modelnames = [#\"test_labels_femina3_gdice.pth\",\n",
    "              #\"test_labels_femina3_gdicefoc.pth\",\n",
    "              #\"test_labels_femina3_dicece.pth\",\n",
    "              \"test_labels_femina3_unetr_dicece.pth\",\n",
    "]\n",
    "              \n",
    "              \n",
    "\n",
    "for modelname in modelnames:\n",
    "    \n",
    "\n",
    "    #dataloaders\n",
    "    dataloaders, size = getSegmentationDataset(dataset=dataset, batch=batchsize, augment=False, training=False, n4=N4, labels=True, eval_augment=False)\n",
    "\n",
    "    #Train loop\n",
    "    best_loss = 1\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    #Model optimizer and scheduler\n",
    "    #model = getUNetForSegmentation()\n",
    "    model = getUNETRForSegmentation()\n",
    "    optimizer = getAdamOptimizer(model, lr)\n",
    "    scheduler = getReducePlateauScheduler(optimizer, patience=patience, factor=factor)\n",
    "    loadExistingModel(model, optimizer, ft, ct)\n",
    "    softmax = Activations(other=nn.Softmax(dim=1))\n",
    "    ptdr = \"dataset3/Atlas/P56_Atlas_128_norm_id.nii.gz\"\n",
    "    affine = nib.load(ptdr).affine\n",
    "    header = nib.load(ptdr).header\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            metrics = []\n",
    "            for i in range(4):\n",
    "                metrics.append([])\n",
    "\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                print(i, end='\\r')\n",
    "                optimizer.zero_grad()      \n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    inputs, labels = data[\"img\"].to(device), data[\"seg\"].to(device)\n",
    "\n",
    "                    outputs = model(inputs)  \n",
    "                    onehot_labels = monai.networks.utils.one_hot(labels, num_classes=4,dim=1)\n",
    "\n",
    "                    if modelname == \"test_labels_femina3_gdice.pth\":\n",
    "                        loss = loss_gdice(outputs, onehot_labels)\n",
    "                    elif modelname == \"test_labels_femina3_gdicefoc.pth\":\n",
    "                        loss = loss_gdicefoc(outputs, onehot_labels)\n",
    "                    elif modelname == \"test_labels_femina3_dicece.pth\" or modelname == \"test_labels_femina3_unetr_dicece.pth\":\n",
    "                        loss = loss_dicece(outputs, onehot_labels)\n",
    "                            \n",
    "                    probs = softmax(outputs)             \n",
    "\n",
    "                    dice_metric = DiceMetric(include_background=True, reduction=\"mean_channel\", get_not_nans=False)\n",
    "                    dice_metric(y_pred=probs.squeeze(), y=onehot_labels.squeeze())\n",
    "                    metric = dice_metric.aggregate()\n",
    "\n",
    "                    for j in range(4):\n",
    "                        metrics[j].append(metric[j].item())                \n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    #if phase=='valid' and i==5:\n",
    "                    #    print(data['img_meta_dict']['filename_or_obj'])\n",
    "                    #    print(data['seg_meta_dict']['filename_or_obj'])\n",
    "                    #    image = inputs.squeeze().detach().cpu().numpy()\n",
    "                    #    plt.imshow(image[64,:,:])\n",
    "                    #    outputs_labels = outputs.detach().cpu().numpy()\n",
    "                    #    outputs_labels = np.argmax(outputs_labels, axis=1).squeeze()\n",
    "                    #    plt.imshow(outputs_labels[64,:,:], cmap='jet', alpha=0.5)\n",
    "                    #    plt.show()\n",
    "                    #    nib.save(nib.Nifti1Image(outputs_labels, affine, header), \"test.nii.gz\")  \n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            running_loss /= size[phase]\n",
    "            metrics_mean = [np.mean(x) for x in metrics]\n",
    "            running_metric = np.mean(metrics_mean)     \n",
    "            print(\n",
    "                \"{}: loss: {:.4f}, dice: {:.4f}\".format(\n",
    "                    phase, running_loss, running_metric\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"dices: {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                    metrics_mean[0], metrics_mean[1], metrics_mean[2], metrics_mean[3]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss            \n",
    "            elif phase == 'valid':\n",
    "                valid_loss = running_loss\n",
    "                scheduler.step(running_loss)\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_epoch = epoch + 1\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict()\n",
    "                    },\n",
    "                        './models/' + str(modelname))\n",
    "\n",
    "                    print(\n",
    "                        \"best loss {:.4f} at epoch {}\".format(\n",
    "                            best_loss, best_epoch\n",
    "                        )\n",
    "                    )            \n",
    "        writer.add_scalars('epoch_loss', {\n",
    "            'train': train_loss,\n",
    "            'valid': valid_loss,\n",
    "        }, epoch + 1)\n",
    "\n",
    "    print(f\"train completed\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34274621",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-ecology",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-montreal",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-portrait",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-christopher",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def get_labels_as_one_hot(seg):\n",
    "    labels=np.zeros([3]+list(seg.shape))\n",
    "    for k in range(int(seg.max())+1):\n",
    "        one_hot=np.zeros_like(seg)        \n",
    "        one_hot[seg==k]=1 \n",
    "        labels[k,:,:,:]=one_hot\n",
    "    return labels\n",
    "\n",
    "path = data['seg_file'][0]\n",
    "print(path)\n",
    "data_raw = nib.load(path).get_fdata()\n",
    "labels_raw = get_labels_as_one_hot(data_raw)\n",
    "\n",
    "print(data_raw.shape)\n",
    "print(labels_raw.shape)\n",
    "\n",
    "a = np.where(labels_raw[0,:,:,:] == 1)\n",
    "b = np.where(labels_raw[1,:,:,:] == 1)\n",
    "c = np.where(labels_raw[2,:,:,:] == 1)\n",
    "d = np.where(labels_raw[3,:,:,:] == 1)\n",
    "e = np.where((labels_raw[0,:,:,:] == 0) & (labels_raw[1,:,:,:] == 0) & (labels_raw[2,:,:,:] == 0) & (labels_raw[3,:,:,:] == 0))\n",
    "\n",
    "maxV = 305*216*227\n",
    "\n",
    "print(len(e[0])/maxV*100)\n",
    "print(len(a[0])/maxV*100)\n",
    "print(len(b[0])/maxV*100)\n",
    "print(len(c[0])/maxV*100)\n",
    "print(len(d[0])/maxV*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-intersection",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from monai.transforms import Resize\n",
    "labels_raw_resize = Resize(spatial_size=(128,128,128))(labels_raw)\n",
    "print(labels_raw_resize.shape)\n",
    "\n",
    "\n",
    "a = np.where(labels_raw_resize[0,:,:,:] == 1)\n",
    "b = np.where(labels_raw_resize[1,:,:,:] == 1)\n",
    "c = np.where(labels_raw_resize[2,:,:,:] == 1)\n",
    "d = np.where(labels_raw_resize[3,:,:,:] == 1)\n",
    "e = np.where((labels_raw_resize[0,:,:,:] == 0) & (labels_raw_resize[1,:,:,:] == 0) & (labels_raw_resize[2,:,:,:] == 0) & (labels_raw_resize[3,:,:,:] == 0))\n",
    "\n",
    "maxV = 128*128*128\n",
    "\n",
    "print(len(e[0])/maxV*100)\n",
    "print(len(a[0])/maxV*100)\n",
    "print(len(b[0])/maxV*100)\n",
    "print(len(c[0])/maxV*100)\n",
    "print(len(d[0])/maxV*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-major",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = next(iter(dataloaders['train']))\n",
    "labels = data['seg']\n",
    "print(data['seg'].shape)\n",
    "print(labels.shape)\n",
    "\n",
    "a = np.where(labels[0,0,0,:,:,:] == 1)\n",
    "b = np.where(labels[0,0,1,:,:,:] == 1)\n",
    "c = np.where(labels[0,0,2,:,:,:] == 1)\n",
    "d = np.where(labels[0,0,3,:,:,:] == 1)\n",
    "e = np.where((labels[0,0,0,:,:,:] == 0) & (labels[0,0,1,:,:,:] == 0) & (labels[0,0,2,:,:,:] == 0) & (labels[0,0,3,:,:,:] == 0))\n",
    "\n",
    "maxV = 128*128*128\n",
    "\n",
    "print(len(e[0])/maxV*100)\n",
    "print(len(a[0])/maxV*100)\n",
    "print(len(b[0])/maxV*100)\n",
    "print(len(c[0])/maxV*100)\n",
    "print(len(d[0])/maxV*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a50c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594a602",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc5175",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
