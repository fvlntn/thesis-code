{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspended-harrison",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import monai\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.utils import set_determinism\n",
    "import numpy as np\n",
    "\n",
    "from seg_data import getDataset\n",
    "from transforms_dict import getSegmentationPostProcessingForLabel, getSegmentationPostProcessingForLabelOutput\n",
    "from utils import compute_mean_dice, getReducePlateauScheduler, getAdamOptimizer, loadExistingModel\n",
    "from utils import print_model_output, check_model_name, getDevice\n",
    "from seg_model import getUNetForSegmentation\n",
    "\n",
    "from monai.transforms import Activations\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import decollate_batch\n",
    "from monai.metrics import DiceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb288e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "modelname = \"test_seg_labels.pth\"\n",
    "dataset = \"painfactlabels\"\n",
    "ft = None\n",
    "ct = None\n",
    "batchsize = 1\n",
    "num_epochs = 200\n",
    "factor = 0.9\n",
    "patience = 10\n",
    "augment = True\n",
    "N4 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531ae899",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving to test_seg_labels.pth\n"
     ]
    }
   ],
   "source": [
    "#Modelname and device\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "modelname = check_model_name(modelname)\n",
    "print_model_output(modelname)\n",
    "set_determinism(seed=0)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "device = getDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d4bcd9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#postprocessing\n",
    "outputs_processing = getSegmentationPostProcessingForLabelOutput()\n",
    "labels_processing = getSegmentationPostProcessingForLabel()\n",
    "\n",
    "#activations\n",
    "softmax = Activations(other=nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c15181",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using Painfact-Segmentation dataset.\n",
      "=> Using augmented transforms for train set\n"
     ]
    }
   ],
   "source": [
    "#dataloaders\n",
    "dataloaders, size = getDataset(dataset=dataset, batch=batchsize, augment=augment, training=True, n4=N4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fe9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric MUNet\n",
    "def metric_munet(preds, labels):  \n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    preds = preds.detach().cpu().numpy()    \n",
    "    labels[np.where(labels == np.amax(labels, axis=0))] = 1\n",
    "    labels[labels != 1] = 0\n",
    "    dice=2*np.sum(labels*preds,(1,2,3))/(np.sum((labels+preds),(1,2,3))+1)    \n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b64c816",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Loss\n",
    "def loss_Dice(preds, labels):\n",
    "    dice = 1-torch.div(\n",
    "        torch.sum(torch.mul(torch.mul(labels,preds),2)),\n",
    "        torch.sum(torch.mul(preds,preds)) + torch.sum(torch.mul(labels,labels))\n",
    "        )    \n",
    "    return dice\n",
    "\n",
    "def loss_CE(input, target):\n",
    "        n_pred_ch, n_target_ch = input.shape[1], target.shape[1]\n",
    "        if n_pred_ch == n_target_ch:\n",
    "            target = torch.argmax(target, dim=1)\n",
    "        else:\n",
    "            target = torch.squeeze(target, dim=1)\n",
    "        target = target.long()        \n",
    "        device = getDevice()\n",
    "        weight = torch.FloatTensor([1.0, 5.0, 5.0, 20.0]).to(device)\n",
    "        return nn.CrossEntropyLoss(reduction=\"mean\", weight=weight)(input, target)\n",
    "\n",
    "loss_GDice = monai.losses.GeneralizedDiceLoss(other_act=nn.Softmax(dim=1))\n",
    "weight = torch.FloatTensor([1.0, 5.0, 5.0, 20.0]).to(device)\n",
    "loss_DiceCE = monai.losses.DiceCELoss(other_act=nn.Softmax(dim=1), ce_weight=weight)\n",
    "\n",
    "def loss_GDiceCE(input, target, lambda_gdice=1.0, lambda_ce=1.0):    \n",
    "    GDice = loss_GDice(input, target)\n",
    "    CE = loss_CE(input, target)    \n",
    "    GDiceCELoss = lambda_gdice*GDice + lambda_ce*CE\n",
    "    return GDiceCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7019bcdd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/200\n",
      "train: loss: 1.6316, dice: 0.5564\n",
      "dices: 0.8353, 0.7898, 0.4775, 0.1230\n",
      "----------\n",
      "epoch 2/200\n",
      "train: loss: 1.4521, dice: 0.6257\n",
      "dices: 0.8757, 0.8273, 0.5901, 0.2099\n",
      "----------\n",
      "epoch 3/200\n",
      "train: loss: 1.3752, dice: 0.6474\n",
      "dices: 0.8995, 0.8350, 0.6154, 0.2397\n",
      "----------\n",
      "epoch 4/200\n",
      "train: loss: 1.3141, dice: 0.6708\n",
      "dices: 0.9249, 0.8434, 0.6552, 0.2595\n",
      "----------\n",
      "epoch 5/200\n",
      "train: loss: 1.2672, dice: 0.6814\n",
      "dices: 0.9293, 0.8530, 0.6693, 0.2740\n",
      "valid: loss: 1.0909, dice: 0.7204\n",
      "dices: 0.9785, 0.8797, 0.7621, 0.2611\n",
      "best loss 1.0909 at epoch 5\n",
      "----------\n",
      "epoch 6/200\n",
      "train: loss: 1.2368, dice: 0.6901\n",
      "dices: 0.9304, 0.8598, 0.6857, 0.2845\n",
      "----------\n",
      "epoch 7/200\n",
      "train: loss: 1.2215, dice: 0.6923\n",
      "dices: 0.9353, 0.8645, 0.6861, 0.2835\n",
      "----------\n",
      "epoch 8/200\n",
      "train: loss: 1.1934, dice: 0.7048\n",
      "dices: 0.9347, 0.8667, 0.7152, 0.3024\n",
      "----------\n",
      "epoch 9/200\n",
      "train: loss: 1.1751, dice: 0.7067\n",
      "dices: 0.9376, 0.8731, 0.7110, 0.3051\n",
      "valid: loss: 1.0028, dice: 0.7427\n",
      "dices: 0.9859, 0.8904, 0.7839, 0.3105\n",
      "best loss 1.0028 at epoch 9\n",
      "----------\n",
      "epoch 10/200\n",
      "train: loss: 1.1639, dice: 0.7077\n",
      "dices: 0.9384, 0.8773, 0.7134, 0.3017\n",
      "----------\n",
      "epoch 11/200\n",
      "train: loss: 1.1512, dice: 0.7100\n",
      "dices: 0.9339, 0.8774, 0.7187, 0.3100\n",
      "----------\n",
      "epoch 12/200\n",
      "train: loss: 1.1299, dice: 0.7154\n",
      "dices: 0.9414, 0.8819, 0.7295, 0.3090\n",
      "----------\n",
      "epoch 13/200\n",
      "train: loss: 1.1070, dice: 0.7185\n",
      "dices: 0.9351, 0.8858, 0.7331, 0.3202\n",
      "valid: loss: 0.9244, dice: 0.7625\n",
      "dices: 0.9839, 0.9077, 0.8162, 0.3424\n",
      "best loss 0.9244 at epoch 13\n",
      "----------\n",
      "epoch 14/200\n",
      "train: loss: 1.0915, dice: 0.7221\n",
      "dices: 0.9387, 0.8868, 0.7415, 0.3213\n",
      "----------\n",
      "epoch 15/200\n",
      "train: loss: 1.0887, dice: 0.7202\n",
      "dices: 0.9414, 0.8903, 0.7279, 0.3214\n",
      "----------\n",
      "epoch 16/200\n",
      "train: loss: 1.0797, dice: 0.7236\n",
      "dices: 0.9434, 0.8917, 0.7337, 0.3255\n",
      "----------\n",
      "epoch 17/200\n",
      "train: loss: 1.0733, dice: 0.7252\n",
      "dices: 0.9400, 0.8925, 0.7417, 0.3264\n",
      "valid: loss: 0.9092, dice: 0.7671\n",
      "dices: 0.9858, 0.9052, 0.8225, 0.3550\n",
      "best loss 0.9092 at epoch 17\n",
      "----------\n",
      "epoch 18/200\n",
      "train: loss: 1.0785, dice: 0.7233\n",
      "dices: 0.9406, 0.8905, 0.7374, 0.3246\n",
      "----------\n",
      "epoch 19/200\n",
      "train: loss: 1.0567, dice: 0.7278\n",
      "dices: 0.9451, 0.8966, 0.7413, 0.3280\n",
      "----------\n",
      "epoch 20/200\n",
      "train: loss: 1.0548, dice: 0.7259\n",
      "dices: 0.9396, 0.8970, 0.7396, 0.3274\n",
      "----------\n",
      "epoch 21/200\n",
      "train: loss: 1.0700, dice: 0.7248\n",
      "dices: 0.9389, 0.8918, 0.7440, 0.3244\n",
      "valid: loss: 0.9021, dice: 0.7673\n",
      "dices: 0.9846, 0.9087, 0.8288, 0.3470\n",
      "best loss 0.9021 at epoch 21\n",
      "----------\n",
      "epoch 22/200\n",
      "train: loss: 1.0395, dice: 0.7326\n",
      "dices: 0.9403, 0.8950, 0.7584, 0.3367\n",
      "----------\n",
      "epoch 23/200\n",
      "train: loss: 1.0506, dice: 0.7266\n",
      "dices: 0.9415, 0.8965, 0.7436, 0.3246\n",
      "----------\n",
      "epoch 24/200\n",
      "train: loss: 1.0316, dice: 0.7361\n",
      "dices: 0.9436, 0.8982, 0.7595, 0.3430\n",
      "----------\n",
      "epoch 25/200\n",
      "train: loss: 1.0332, dice: 0.7348\n",
      "dices: 0.9466, 0.8990, 0.7540, 0.3396\n",
      "valid: loss: 0.9009, dice: 0.7699\n",
      "dices: 0.9856, 0.9097, 0.8315, 0.3529\n",
      "best loss 0.9009 at epoch 25\n",
      "----------\n",
      "epoch 26/200\n",
      "train: loss: 1.0239, dice: 0.7354\n",
      "dices: 0.9450, 0.8979, 0.7624, 0.3364\n",
      "----------\n",
      "epoch 27/200\n",
      "train: loss: 1.0200, dice: 0.7363\n",
      "dices: 0.9461, 0.9008, 0.7582, 0.3402\n",
      "----------\n",
      "epoch 28/200\n",
      "train: loss: 1.0235, dice: 0.7322\n",
      "dices: 0.9425, 0.8988, 0.7505, 0.3368\n",
      "----------\n",
      "epoch 29/200\n",
      "train: loss: 1.0101, dice: 0.7378\n",
      "dices: 0.9457, 0.9027, 0.7588, 0.3441\n",
      "valid: loss: 0.8682, dice: 0.7781\n",
      "dices: 0.9866, 0.9171, 0.8359, 0.3727\n",
      "best loss 0.8682 at epoch 29\n",
      "----------\n",
      "epoch 30/200\n",
      "train: loss: 1.0261, dice: 0.7331\n",
      "dices: 0.9411, 0.8997, 0.7518, 0.3397\n",
      "----------\n",
      "epoch 31/200\n",
      "train: loss: 1.0123, dice: 0.7374\n",
      "dices: 0.9425, 0.8997, 0.7642, 0.3432\n",
      "----------\n",
      "epoch 32/200\n",
      "train: loss: 1.0114, dice: 0.7365\n",
      "dices: 0.9446, 0.9018, 0.7586, 0.3409\n",
      "----------\n",
      "epoch 33/200\n",
      "train: loss: 1.0191, dice: 0.7306\n",
      "dices: 0.9409, 0.9018, 0.7465, 0.3333\n",
      "valid: loss: 0.8914, dice: 0.7698\n",
      "dices: 0.9828, 0.9172, 0.8254, 0.3539\n",
      "----------\n",
      "epoch 34/200\n",
      "train: loss: 0.9963, dice: 0.7422\n",
      "dices: 0.9490, 0.9037, 0.7626, 0.3537\n",
      "----------\n",
      "epoch 35/200\n",
      "100/166\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6f6f78ca60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/valentini/dev/Mousenet/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/valentini/dev/Mousenet/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1322, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train loop\n",
    "best_loss = np.inf\n",
    "writer = SummaryWriter()\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#Model optimizer and scheduler\n",
    "model = getUNetForSegmentation()\n",
    "lr = 1e-3#/np.sqrt(6)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = getReducePlateauScheduler(optimizer, patience=patience, factor=factor)\n",
    "loadExistingModel(model, optimizer, ft, ct)\n",
    "\n",
    "train_dices = []\n",
    "valid_dices = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    for phase in ['train', 'valid']:\n",
    "        model.train()\n",
    "        \n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0        \n",
    "        metrics = []\n",
    "        for i in range(4):\n",
    "            metrics.append([])\n",
    "        \n",
    "        if phase == 'train' or (epoch % 4 == 0 and epoch != 0):\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                print(\"{}/{}\".format(\n",
    "                    i, len(dataloaders[phase])), end='\\r'\n",
    "                )\n",
    "                optimizer.zero_grad()            \n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    inputs, labels = data[\"img\"].to(device), data[\"seg\"].to(device)\n",
    "                    labels = labels.squeeze(2) \n",
    "\n",
    "                    if phase == 'train':        \n",
    "                        outputs = model(inputs)\n",
    "                    else:\n",
    "                        outputs = sliding_window_inference(inputs, (96, 96, 96), 4, model)                \n",
    "                    loss = loss_GDiceCE(outputs, labels)\n",
    "                    preds = [outputs_processing(pred) for pred in decollate_batch(outputs)]\n",
    "                    labels = [labels_processing(label) for label in decollate_batch(labels)]  \n",
    "                    for j in range(len(preds)):\n",
    "                        metric = metric_munet(preds[j], labels[j])                     \n",
    "                        for k in range(4):\n",
    "                            metrics[k].append(metric[k])                \n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            running_loss /= size[phase]\n",
    "            metrics_mean = [np.mean(x) for x in metrics]\n",
    "            running_metric = np.mean(metrics_mean)      \n",
    "\n",
    "            print(\n",
    "                \"{}: loss: {:.4f}, dice: {:.4f}\".format(\n",
    "                    phase, running_loss, running_metric\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"dices: {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                    metrics_mean[0], metrics_mean[1], metrics_mean[2], metrics_mean[3]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss               \n",
    "                train_dices.append(running_metric)\n",
    "            elif phase == 'valid':\n",
    "                valid_loss = running_loss \n",
    "                valid_dices.append(running_metric)\n",
    "                scheduler.step(running_loss)\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_epoch = epoch + 1\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict()\n",
    "                    },\n",
    "                        './models/' + str(modelname))\n",
    "\n",
    "                    print(\n",
    "                        \"best loss {:.4f} at epoch {}\".format(\n",
    "                            best_loss, best_epoch\n",
    "                        )\n",
    "                    )            \n",
    "    writer.add_scalars('epoch_loss', {\n",
    "        'train': train_loss,\n",
    "        'valid': valid_loss,\n",
    "    }, epoch + 1)\n",
    "    \n",
    "\n",
    "print(f\"train completed\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
