{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fce49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import subprocess\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import scipy.ndimage\n",
    "import scipy\n",
    "import torch\n",
    "from skimage.measure import regionprops\n",
    "from monai.networks.blocks import Warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca99ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRegionsAsCSV_antss(mask, out):\n",
    "    ants_command = \"ImageMath 3 \" + str(out) + \" LabelStats \" + str(mask) + \" \" + str(mask)\n",
    "    subprocess.call(ants_command.split(\" \"))  \n",
    "    \n",
    "def getRegionsAsCSV_scipy(mask, out):    \n",
    "    img = nib.load(mask).get_fdata().astype(np.uint8)\n",
    "    regions = regionprops(img)\n",
    "    df = pd.DataFrame()\n",
    "    for i, props in enumerate(regions):\n",
    "        x, y, z = props.centroid\n",
    "        x = x # + 1 # +1 because ITK-Snap is between 1-128 and Python 0-127\n",
    "        y = y # + 1\n",
    "        z = z # + 1\n",
    "        t = 0\n",
    "        label = props.label\n",
    "        mass = props.area\n",
    "        volume = props.area\n",
    "        count = len(props.coords)\n",
    "        row = {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'z': z,\n",
    "            't': t,\n",
    "            'label': label,\n",
    "            'mass': mass,\n",
    "            'volume': volume,\n",
    "            'count': count,\n",
    "        }\n",
    "        temp_df = pd.DataFrame(data=row, index=[0])\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    df.to_csv(out, index=False)\n",
    "    \n",
    "def get_csv_positive(csv, out):\n",
    "    df = pd.read_csv(csv, sep=',', header=0)\n",
    "    df_out = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        row = {\n",
    "            'x': np.abs(df['x'][i]),\n",
    "            'y': np.abs(df['y'][i]),\n",
    "            'z': np.abs(df['z'][i]),\n",
    "            't': df['t'][i],\n",
    "            'label': df['label'][i],\n",
    "            'mass': df['mass'][i],\n",
    "            'volume': df['volume'][i],\n",
    "            'count': df['count'][i],\n",
    "        }  \n",
    "        temp_df = pd.DataFrame(data=row, index=[0])\n",
    "        df_out = pd.concat([df_out, temp_df], ignore_index=True)\n",
    "    df_out.to_csv(out, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a59ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = os.path.join(\"/home/valentini/dev/Mousenet/dataset3/Atlas/\", \"P56_Atlas_128_norm_id.nii.gz\")\n",
    "out_atlas_antss = os.path.join(\"dataset3\", \"Atlas\", \"P56_Atlas_landmarks_id_ants.csv\")\n",
    "out_atlas_scipy = os.path.join(\"dataset3\", \"Atlas\", \"P56_Atlas_landmarks_id_scipy.csv\")\n",
    "atlas_landmark = os.path.join(\"dataset3\", \"Atlas\", \"P56_Atlas_landmarks_id.nii.gz\")\n",
    "#getRegionsAsCSV_antss(atlas_landmark, out_atlas_antss)       \n",
    "#getRegionsAsCSV_scipy(atlas_landmark, out_atlas_scipy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb962bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset in [\"Feminad\"]:\n",
    "#    print(dataset)\n",
    "#    data_dir = os.path.join(\"/home/valentini/dev/Mousenet/dataset2/\" + str(dataset) + \"/\") \n",
    "#    landmarks_origin = sorted(glob(os.path.join(data_dir, \"Landmarks_Resample\", \"*.nii.gz\")))\n",
    "#    landmarks_identi = sorted(glob(os.path.join(data_dir, \"Landmarks_Resample_Identity\", \"*.nii.gz\")))\n",
    "#    landmarks_affine = sorted(glob(os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine\", \"*.nii.gz\")))\n",
    "#    landmarks_deform = sorted(glob(os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine_Deformable\", \"*.nii.gz\")))\n",
    "#    for i in range(len(landmarks_origin)):\n",
    "#                        \n",
    "#        out_identi_antss = os.path.join(data_dir, \"Landmarks_Resample_Identity\", landmarks_identi[i].split('/')[-1].split('.')[0] + '_antss.csv')\n",
    "#        out_identi_scipy = os.path.join(data_dir, \"Landmarks_Resample_Identity\", landmarks_identi[i].split('/')[-1].split('.')[0] + '_scipy.csv')\n",
    "#        getRegionsAsCSV_antss(landmarks_identi[i], out_identi_antss)       \n",
    "#        getRegionsAsCSV_scipy(landmarks_identi[i], out_identi_scipy)\n",
    "#                \n",
    "#        name = landmarks_origin[i].split('/')[-1].split('.')[0]\n",
    "#        name = \"_\" + name.split('_')[-2] + '_' + name.split('_')[-1]\n",
    "#        \n",
    "#        mat = glob(os.path.join(data_dir, 'MRI_N4_Resample_Norm_Identity_Affine_Deformable', \"*\" + name + \"*.mat\"))[0]     \n",
    "#        out_affine_apply = os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine\", landmarks_affine[i].split('/')[-1].split('.')[0] + '_apply.csv')\n",
    "#        ants_command = \"antsApplyTransformsToPoints -d 3 -i \" + str(out_identi_antss) + \" -o \" + str(out_affine_apply) + \" -t [\" + str(mat) + \",1]\"\n",
    "#        subprocess.call(ants_command.split(\" \"))  \n",
    "#        \n",
    "#        out_affine_apply_2 = os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine\", landmarks_affine[i].split('/')[-1].split('.')[0] + '_apply_pos.csv')\n",
    "#        get_csv_positive(out_affine_apply, out_affine_apply_2)\n",
    "#        \n",
    "#        invwarp = sorted(glob(os.path.join(data_dir, 'MRI_N4_Resample_Norm_Identity_Affine_Deformable', \"*\" + name + \"*InverseWarp.nii.gz\")))[0] \n",
    "#        warp = sorted(glob(os.path.join(data_dir, 'MRI_N4_Resample_Norm_Identity_Affine_Deformable', \"*\" + name + \"*Warp.nii.gz\")))[1]\n",
    "#        \n",
    "#        out_deform_apply2 = os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine_Deformable\", landmarks_deform[i].split('/')[-1].split('.')[0] + '_applytoapply.csv')\n",
    "#        ants_command = \"antsApplyTransformsToPoints -d 3 -i \" + str(out_affine_apply) + \" -o \" + str(out_deform_apply2) + \" -t \" + str(invwarp)\n",
    "#        subprocess.call(ants_command.split(\" \"))\n",
    "#        \n",
    "#        out_deform_apply3 = os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine_Deformable\", landmarks_deform[i].split('/')[-1].split('.')[0] + '_applytoapply_pos.csv')\n",
    "#        get_csv_positive(out_deform_apply2, out_deform_apply3)\n",
    "#        \n",
    "#        out_deform_apply_atlas = os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine\", landmarks_affine[i].split('/')[-1].split('.')[0] + '_atlas_ants.csv')\n",
    "#        ants_command = \"antsApplyTransformsToPoints -d 3 -i \" + str(out_atlas_antss) + \" -o \" + str(out_deform_apply_atlas) + \" -t \" + str(warp)\n",
    "#        subprocess.call(ants_command.split(\" \"))\n",
    "#                \n",
    "#        #out_deform_apply_atlas_2 = os.path.join(data_dir, \"Landmarks_Resample_Identity_Affine\", landmarks_affine[i].split('/')[-1].split('.')[0] + '_atlas_applytoapply_pos.csv')\n",
    "#        #get_csv_positive(out_deform_apply_atlas, out_deform_apply_atlas_2)\n",
    "#        \n",
    "#    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b7054",
   "metadata": {},
   "source": [
    "## Get DL Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ba2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_warp(warp_data, point):\n",
    "    x_range = np.arange(0, warp_data.shape[1], 1)\n",
    "    y_range = np.arange(0, warp_data.shape[2], 1)\n",
    "    z_range = np.arange(0, warp_data.shape[3], 1)\n",
    "\n",
    "    interp_x = scipy.interpolate.RegularGridInterpolator((x_range, y_range, z_range), warp_data[0, :, :, :],\n",
    "                                                         fill_value=0)\n",
    "    dx = interp_x(point)\n",
    "\n",
    "    interp_y = scipy.interpolate.RegularGridInterpolator((x_range, y_range, z_range), warp_data[1, :, :, :],\n",
    "                                                         fill_value=0)\n",
    "    dy = interp_y(point)\n",
    "\n",
    "    interp_z = scipy.interpolate.RegularGridInterpolator((x_range, y_range, z_range), warp_data[2, :, :, :],\n",
    "                                                         fill_value=0)\n",
    "    dz = interp_z(point)\n",
    "\n",
    "    return dx, dy, dz\n",
    "\n",
    "def create_row_csv(x, y, z, t, label, mass, volume, count):\n",
    "    row = {\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'z': z,\n",
    "        't': t,\n",
    "        'label': label,\n",
    "        'mass': mass,\n",
    "        'volume': volume,\n",
    "        'count': count,\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def apply_warp_to_landmarks_df(df, warp):\n",
    "    # DDF in [N,N,N,1,3] -> [3,N,N,N]\n",
    "    if isinstance(warp, str):\n",
    "        warp = nib.load(warp)\n",
    "        warp_data = torch.from_numpy(warp.get_fdata())\n",
    "    elif isinstance(warp, MetaTensor):\n",
    "        warp_data = warp\n",
    "    else:\n",
    "        warp_data = torch.from_numpy(warp.get_fdata())\n",
    "    warp_data = warp_data.permute(4,0,1,2,3).squeeze().cpu().detach().numpy()\n",
    "    if isinstance(df, str):\n",
    "        df = pd.read_csv(df)\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        x = df['x'][i]\n",
    "        y = df['y'][i]\n",
    "        z = df['z'][i]\n",
    "\n",
    "        dx, dy, dz = interpolate_warp(warp_data, (x, y, z))\n",
    "        \n",
    "        #here -dx and -dy because we saved ddf in ants format (with *[-1,-1,0])\n",
    "        new_x = x - dx\n",
    "        new_y = y - dy\n",
    "        new_z = z + dz\n",
    "\n",
    "        t = df['t'][i]\n",
    "        label = df['label'][i]\n",
    "        mass = df['mass'][i]\n",
    "        volume = df['volume'][i]\n",
    "        count = df['count'][i]\n",
    "\n",
    "        row = create_row_csv(new_x, new_y, new_z, t, label, mass, volume, count)\n",
    "\n",
    "        temp_df = pd.DataFrame(data=row, index=[0])\n",
    "        new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38aac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.blocks.Warp: Using PyTorch native grid_sample.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33\r"
     ]
    }
   ],
   "source": [
    "warp_nearest = Warp(\"nearest\")\n",
    "for dataset in [\"Feminad\"]:\n",
    "    data_dir = os.path.join(\"/home/valentini/dev/Mousenet/dataset3/\" + str(dataset) + \"/\")\n",
    "    affine_csvs = sorted(glob(os.path.join(data_dir, \"Landmarks\", \"*_affpos.csv\")))\n",
    "    for output_folder in [\n",
    "              \"feminad-sym-1\",\n",
    "              \"feminad-sym-8\",\n",
    "             ]:\n",
    "        landmarks_folder = os.path.join(\"output\", \"Feminad\", output_folder, \"Landmarks_Deformable\")\n",
    "        deform_warps = sorted(glob(os.path.join(\"output\", \"Feminad\", output_folder, \"DeformableWarp\", \"*.nii.gz\")))\n",
    "        try:\n",
    "            os.mkdir(landmarks_folder)\n",
    "        except Exception:\n",
    "            pass         \n",
    "        for i, csv in enumerate(affine_csvs):\n",
    "            print(str(i+1) + \"/\" + str(len(affine_csvs)), end='\\r')\n",
    "            name = csv.split('/')[-1].split('.')[0]\n",
    "            name = name.split('_')[0] + '_' + name.split('_')[1]\n",
    "            deform_warp = deform_warps[i]\n",
    "            \n",
    "            out_csv_atlas = os.path.join(landmarks_folder, \"Affine_\" + name + '_atlas.csv')\n",
    "            apply_warp_to_landmarks_df(out_atlas_scipy, deform_warp).to_csv(out_csv_atlas, index=False)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43fa81",
   "metadata": {},
   "source": [
    "## Compute Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f737c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csv_distance(csv1, csv2):\n",
    "    if isinstance(csv1, str):\n",
    "        csv1 = pd.read_csv(csv1, sep=',', header=0)\n",
    "    if isinstance(csv2, str):\n",
    "        csv2 = pd.read_csv(csv2, sep=',', header=0)\n",
    "    TREs = []\n",
    "    for i in csv2['label']:\n",
    "        if True:\n",
    "            ind1 = np.where(csv1['label'] == i)[0]\n",
    "            ind2 = np.where(csv2['label'] == i)[0]\n",
    "            if len(ind1) == 1 and len(ind2) == 1:\n",
    "                x1 = csv1['x'][ind1[0]]\n",
    "                x2 = csv2['x'][ind2[0]]\n",
    "\n",
    "                y1 = csv1['y'][ind1[0]]\n",
    "                y2 = csv2['y'][ind2[0]]\n",
    "\n",
    "                z1 = csv1['z'][ind1[0]]\n",
    "                z2 = csv2['z'][ind2[0]]\n",
    "\n",
    "                TRE = np.sqrt((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)+(z1-z2)*(z1-z2))\n",
    "                TREs.append(TRE)\n",
    "    return TREs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8115b414",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda1_0.001 - 31.966334436703317 - 0.931578258486779\n",
      "lambda1_0.01 - 29.141472785019765 - 1.0455574776124805\n",
      "lambda1_0.1 - 3.5189470711006217 - 0.8135360261722824\n",
      "lambda1_1.0 - 2.869457274790764 - 0.7000282160789857\n",
      "lambda1_10 - 2.8854478758141577 - 0.7302917058655005\n",
      "lambda1_100 - 3.577687011055823 - 0.8744040220581096\n",
      "lambda1_1000 - 3.7731739400647704 - 0.9269059051238354\n",
      "scenario1-final - 2.778124207827283 - 0.7226061397678561\n",
      "scenario2-final - 2.903131910227366 - 0.7142292005744381\n",
      "scenario3-final - 2.878842531230274 - 0.7094434315746156\n",
      "feminad-sym-1 - 3.820854776002534 - 1.3973309790218464\n",
      "feminad-sym-8 - 3.661152420936677 - 1.3504505306651176\n",
      "ants - 2.9413065951480224 - 1.0414585258034161\n",
      "affine - 3.5469232304627183 - 0.8772366850714087\n",
      "original - 21.072112832313785 - 2.813962657620705\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"Feminad\"]:\n",
    "\n",
    "    for output_folder in [\n",
    "              \"lambda1_0.001\",\n",
    "              \"lambda1_0.01\",\n",
    "              \"lambda1_0.1\",\n",
    "              \"lambda1_1.0\",\n",
    "              \"lambda1_10\",\n",
    "              \"lambda1_100\",\n",
    "              \"lambda1_1000\",\n",
    "              \"scenario1-final\",\n",
    "              \"scenario2-final\",\n",
    "              \"scenario3-final\",\n",
    "              \"feminad-sym-1\",\n",
    "              \"feminad-sym-8\",\n",
    "    ]:\n",
    "        \n",
    "        original_vals = [[], [], [], [], [], []]\n",
    "        affine_vals = [[], [], [], [], [], []]\n",
    "        ants_vals = [[], [], [], [], [], []]\n",
    "        output_vals = [[], [], [], [], [], []]\n",
    "        \n",
    "        landmarks_deep_atlas = sorted(glob(os.path.join(\"output\", \"Feminad\", output_folder, \"Landmarks_Deformable\", \"*_atlas.csv\")))\n",
    "                       \n",
    "        landmarks_original_apply = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_og.csv\")))\n",
    "        landmarks_original_apply_pos = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_ogpos.csv\")))\n",
    "\n",
    "        landmarks_affine_apply = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_aff.csv\")))\n",
    "        landmarks_affine_apply_pos = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_affpos.csv\")))\n",
    "\n",
    "        landmarks_ants_atlas = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_atlasdef.csv\"))) \n",
    "        landmarks_ants_atlaspos = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_atlasdefpos.csv\"))) \n",
    "        \n",
    "        #print(landmarks_deep_atlas)\n",
    "\n",
    "        landmarks_template_antss = os.path.join(\"dataset3\", \"Atlas\", \"P56_Atlas_landmarks_id_ants.csv\")\n",
    "        landmarks_template_scipy = os.path.join(\"dataset3\", \"Atlas\", \"P56_Atlas_landmarks_id_scipy.csv\")\n",
    "        \n",
    "        for i, landmark in enumerate(landmarks_affine_apply):\n",
    "            \n",
    "            original_apply = compute_csv_distance(landmarks_original_apply[i], landmarks_template_antss)            \n",
    "            affine_apply = compute_csv_distance(landmarks_affine_apply[i], landmarks_template_antss)\n",
    "            ants_atlas = compute_csv_distance(landmarks_affine_apply[i], landmarks_ants_atlas[i])            \n",
    "            deep_atlas = compute_csv_distance(landmarks_affine_apply_pos[i], landmarks_deep_atlas[i])\n",
    "            \n",
    "            for i in range(len(original_apply)):\n",
    "                original_vals[i].append(original_apply[i])\n",
    "                affine_vals[i].append(affine_apply[i])\n",
    "                ants_vals[i].append(ants_atlas[i])\n",
    "                output_vals[i].append(deep_atlas[i])        \n",
    "                \n",
    "        means = []\n",
    "        stds = []\n",
    "        #output_vals = np.array(output_vals)\n",
    "        #output_vals = output_vals[1:6,:]\n",
    "        #print(output_folder)\n",
    "        #print(np.mean(output_vals,0))\n",
    "        for val in output_vals:\n",
    "            means.append(np.mean(val))\n",
    "            stds.append(np.std(val))\n",
    "        means = means[1:6]\n",
    "        stds = stds[1:6]\n",
    "        #print(means)\n",
    "        #print(stds)\n",
    "        #print(output_folder + ' - ' + str(means) + ' - ' + str(stds))   \n",
    "        print(output_folder + ' - ' + str(np.mean(means)) + ' - ' + str(np.mean(stds)))      \n",
    "    names = [\"ants\", \"affine\", \"original\"]\n",
    "    for i, liste in enumerate([ants_vals, affine_vals, original_vals]):            \n",
    "        means = []\n",
    "        stds = []\n",
    "        for val in liste:\n",
    "            means.append(np.mean(val))\n",
    "            stds.append(np.std(val))\n",
    "        means = means[1:6]\n",
    "        stds = stds[1:6]\n",
    "        #print(names[i])\n",
    "        #print(means)\n",
    "        #print(stds)\n",
    "        #print(names[i] + ' - ' + str(means) + ' - ' + str(stds))  \n",
    "        print(names[i] + ' - ' + str(np.mean(means)) + ' - ' + str(np.mean(stds)))  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65628bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e3002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda1_0.001\n",
      "[51.01070705674939, 37.864855609996155, 29.26920236393829, 13.57711875199628, 28.074136111305865]\n",
      "lambda1_0.01\n",
      "[49.053984318193734, 35.431126917481016, 26.499286074988518, 23.87249609300217, 10.555083485756427]\n",
      "lambda1_0.1\n",
      "[5.455809223980501, 1.5928976444347016, 1.8808002829830166, 1.9078989487497486, 2.0057307630705115]\n",
      "lambda1_1.0\n",
      "[3.5331696782702493, 1.6507441086573098, 1.6602327622634068, 1.7656589902875082, 2.25811723655985]\n",
      "lambda1_10\n",
      "[3.1020918752971784, 1.5199767063889764, 1.631083720057024, 2.2057133866324112, 2.439064237861165]\n",
      "lambda1_100\n",
      "[3.385433937406982, 2.170256734571169, 2.4432917930096325, 3.753442693383559, 4.020586321562468]\n",
      "lambda1_1000\n",
      "[2.906158515377511, 2.092670885128756, 2.7213352914128373, 4.359094576702778, 4.616174391337452]\n",
      "scenario1-final\n",
      "[3.0694454190926033, 1.5123403290305377, 1.5988052978782135, 1.970313420550443, 2.2213181572691614]\n",
      "feminad-sym-8\n",
      "[3.0137911709591982, 1.9777172152447167, 2.439506915333444, 4.05570321204753, 4.128648521704318]\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"Feminad\"]:\n",
    "\n",
    "    for output_folder in [\n",
    "              \"lambda1_0.001\",\n",
    "              \"lambda1_0.01\",\n",
    "              \"lambda1_0.1\",\n",
    "              \"lambda1_1.0\",\n",
    "              \"lambda1_10\",\n",
    "              \"lambda1_100\",\n",
    "              \"lambda1_1000\",\n",
    "              \"scenario1-final\",\n",
    "              #\"scenario2-final\",\n",
    "              #\"scenario3-final\",\n",
    "              #\"feminad-sym-1\",\n",
    "              \"feminad-sym-8\",\n",
    "    ]:\n",
    "        \n",
    "        original_vals = [[], [], [], [], [], []]\n",
    "        affine_vals = [[], [], [], [], [], []]\n",
    "        ants_vals = [[], [], [], [], [], []]\n",
    "        output_vals = [[], [], [], [], [], []]\n",
    "        landmarks_deep_atlas = sorted(glob(os.path.join(\"output\", \"Feminad\", output_folder, \"Landmarks_Deformable\", \"*_atlasfv.csv\")))\n",
    "                       \n",
    "        landmarks_original_apply = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_og.csv\")))\n",
    "        landmarks_original_apply_pos = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_ogpos.csv\")))\n",
    "\n",
    "        landmarks_affine_apply = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_aff.csv\")))\n",
    "        landmarks_affine_apply_pos = sorted(glob(os.path.join(\"dataset3\", \"Feminad\", \"Landmarks\", \"*_affpos.csv\")))\n",
    "\n",
    "        landmarks_ants_atlas = sorted(glob(os.path.join(\"dataset3\", \"Feminad_Mean\", \"Landmarks\", \"*_atlasfvdef.csv\"))) \n",
    "        landmarks_ants_atlaspos = sorted(glob(os.path.join(\"dataset3\", \"Feminad_Mean\", \"Landmarks\", \"*_atlasfvdefpos.csv\"))) \n",
    "\n",
    "        landmarks_template_antss = os.path.join(\"dataset3\", \"Atlas\", \"P56_Atlas_landmarks_FV_id_ants.csv\")\n",
    "        \n",
    "        for i, landmark in enumerate(landmarks_affine_apply):\n",
    "            \n",
    "            original_apply = compute_csv_distance(landmarks_original_apply[i], landmarks_template_antss)            \n",
    "            affine_apply = compute_csv_distance(landmarks_affine_apply[i], landmarks_template_antss)\n",
    "            ants_atlas = compute_csv_distance(landmarks_affine_apply[i], landmarks_ants_atlas[i])            \n",
    "            deep_atlas = compute_csv_distance(landmarks_affine_apply_pos[i], landmarks_deep_atlas[i])\n",
    "            \n",
    "            for i in range(len(original_apply)):\n",
    "                original_vals[i].append(original_apply[i])\n",
    "                affine_vals[i].append(affine_apply[i])\n",
    "                ants_vals[i].append(ants_atlas[i])\n",
    "                output_vals[i].append(deep_atlas[i])        \n",
    "                \n",
    "        means = []\n",
    "        stds = []\n",
    "        for val in output_vals:\n",
    "            means.append(np.mean(val))\n",
    "            stds.append(np.std(val))\n",
    "        means = means[1:6]\n",
    "        stds = stds[1:6]\n",
    "        print(output_folder)\n",
    "        print(means)\n",
    "        #print(output_folder + ' - ' + str(means) + ' - ' + str(stds))     \n",
    "        #print(output_folder + ' - ' + str(np.mean(means)) + ' - ' + str(np.mean(stds)))   \n",
    "    names = [\"ants\", \"affine\", \"original\"]\n",
    "    #for i, liste in enumerate([ants_vals, affine_vals, original_vals]):            \n",
    "    #    means = []\n",
    "    #    stds = []\n",
    "    #    for val in liste:\n",
    "    #        means.append(np.mean(val))\n",
    "    #        stds.append(np.std(val))        \n",
    "    #    means = means[1:6]\n",
    "    #    stds = stds[1:6]\n",
    "    #    #print(names[i] + ' - ' + str(means) + ' - ' + str(stds))  \n",
    "    #    print(names[i] + ' - ' + str(np.mean(means)) + ' - ' + str(np.mean(stds)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1_0.001 - 31.959203978797195 - 0.9328134737368637\n",
    "lambda1_0.01 - 29.08239537788437 - 1.0314044549371026\n",
    "lambda1_0.1 - 2.5686273726436957 - 0.7726743581931246\n",
    "lambda1_1.0 - 2.1735845552076647 - 0.7118793627565979\n",
    "lambda1_10 - 2.179585985247351 - 0.7591155726078209\n",
    "lambda1_100 - 3.154602295986762 - 0.8578874299380533\n",
    "lambda1_1000 - 3.339086731991867 - 0.8887409659368842\n",
    "scenario1-final - 2.074444524764192 - 0.7537320502894346\n",
    "scenario2-final - 2.1808060413345096 - 0.7478311830632397\n",
    "scenario3-final - 2.1696611992521873 - 0.7424738803593313\n",
    "ants - 2.560370631247044 - 0.7888005771213398\n",
    "affine - 3.0201340831860612 - 0.862368130215337\n",
    "original - 20.469665982422107 - 2.8179641100049286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49561a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario1-final - [6.5878438344080585, 1.5123403290305377, 1.5988052978782135, 1.970313420550443, 2.2213181572691614] - [0.8914072774541518, 0.7137436777322729, 0.6253148536275893, 0.8312285825117052, 0.5513363075135613]\n",
    "scenario2-final - [6.774895849309567, 1.465030623355497, 1.6152106717630432, 2.1351770268371033, 2.5253453798716197] - [0.8811999719012064, 0.7043029984513115, 0.6380589962740504, 0.8093591533607786, 0.5382248828848436]\n",
    "scenario3-final - [6.650890157591889, 1.549108442915863, 1.611179004555159, 2.152562144202721, 2.430472906885737] - [0.8852039913801888, 0.700335999125299, 0.6635909988962887, 0.7913784681360218, 0.5067077003352796]\n",
    "ants - [3.370438071499331, 1.769509809151483, 2.322182013352659, 4.2048299741948245, 3.039573107541815] - [0.9641717997956609, 0.7593787168598513, 1.2437284675940392, 1.41945954522733, 0.8205540995402001]\n",
    "affine - [5.608334863531274, 1.841422897568715, 2.3066097505865324, 3.9236349040985066, 4.054613736528563] - [1.0502172756508101, 0.7839124805474929, 0.912029660111654, 0.9511467945561554, 0.6888772144909318]\n",
    "original - [17.03725706225109, 17.307953884339863, 19.525314660564106, 25.582794914058454, 25.90724364035541] - [2.796923467970565, 2.7488078249991856, 2.5354987818232417, 2.924193464107327, 3.064389749203205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1_0.001 - [51.046359346279964, 37.864855609996155, 29.26920236393829, 13.57711875199628, 28.074136111305865] - [1.1010818760075238, 1.0813650529789733, 1.1195845069393024, 0.6817024360731896, 0.6741574204349057]\n",
    "lambda1_0.01 - [49.34937135387071, 35.431126917481016, 26.499286074988518, 23.87249609300217, 10.555083485756427] - [1.0573208436683577, 1.1273376933019814, 1.3881360073061189, 0.9488527757238596, 0.7061400680620852]\n",
    "lambda1_0.1 - [10.207407716265129, 1.5928976444347016, 1.8808002829830166, 1.9078989487497486, 2.0057307630705115] - [1.1801123449308357, 0.6964035406010131, 0.8831246933567831, 0.8899821963376857, 0.4180573556350943]\n",
    "lambda1_1.0 - [7.012533276185746, 1.6507441086573098, 1.6602327622634068, 1.7656589902875082, 2.25811723655985] - [0.9523287546433415, 0.6920191586174759, 0.6669454065705062, 0.73524151366816, 0.45360624689544454]\n",
    "lambda1_10 - [6.631401328131212, 1.5199767063889764, 1.631083720057024, 2.2057133866324112, 2.439064237861165] - [0.8820022020511263, 0.7225225074446215, 0.6334717767781383, 0.8592440316030459, 0.5542180114505704]\n",
    "lambda1_100 - [5.500857512752287, 2.170256734571169, 2.4432917930096325, 3.753442693383559, 4.020586321562468] - [1.0736428271007479, 0.8209741575796928, 0.8176741633572873, 0.9420852470012151, 0.7176437152516052]\n",
    "lambda1_1000 - [5.076594555742029, 2.092670885128756, 2.7213352914128373, 4.359094576702778, 4.616174391337452] - [1.0668027151049542, 0.8246096363203339, 1.0770944104121116, 0.9472037081148217, 0.7188190556669557]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1_0.001 - 31.966334436703317 - 0.931578258486779\n",
    "lambda1_0.01 - 29.141472785019765 - 1.0455574776124805\n",
    "lambda1_0.1 - 3.5189470711006217 - 0.8135360261722824\n",
    "lambda1_1.0 - 2.869457274790764 - 0.7000282160789857\n",
    "lambda1_10 - 2.8854478758141577 - 0.7302917058655005\n",
    "lambda1_100 - 3.577687011055823 - 0.8744040220581096\n",
    "lambda1_1000 - 3.7731739400647704 - 0.9269059051238354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario1-0.1: 3.3148086915925483\n",
    "scenario2-0.1: 4.786604332016063\n",
    "scenario3-0.1: 4.92144044156853\n",
    "scenario1-8.0: 2.785438264714081\n",
    "scenario2-8.0: 2.8608613936924794\n",
    "scenario3-8.0: 2.7818941269866193\n",
    "ants2: 2.9413065951480237\n",
    "affine: 3.5469232304627174\n",
    "original: 21.072112832313785"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366425de",
   "metadata": {},
   "source": [
    "# VÃ©rification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ab2ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 128, 1, 3)\n",
      "[  0.   0. -14.]\n"
     ]
    }
   ],
   "source": [
    "custom_test = np.zeros((128,128,128,1,3))\n",
    "for i in range(128):\n",
    "    custom_test[:,:,i,0,2] = i-64\n",
    "custom_ddf = nib.Nifti1Image(custom_test, nib.load(ref).affine, nib.load(ref).header)  \n",
    "nib.save(custom_ddf, \"dataset2/Feminad/Custom_DDF.nii.gz\") \n",
    "print(custom_ddf.shape)\n",
    "print(custom_ddf.get_fdata()[50,50,50,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec09941c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/valentini/dev/Mousenet/dataset2/Feminad/Landmarks_Resample_Identity_Affine/Affine_Identity_Landmarks_12_6517_apply_pos.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.blocks.Warp: Using PyTorch native grid_sample.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDF: tensor([  0.,   0., -14.], dtype=torch.float64)\n",
      "Original:\n",
      "      x     y      z  t  label  mass  volume  count\n",
      "0  51.0   4.0   71.0  0      1     1       1      1\n",
      "1  53.0  43.0  108.0  0      2     1       1      1\n",
      "2  52.0  53.0   99.0  0      3     1       1      1\n",
      "3  52.0  65.0   96.0  0      4     1       1      1\n",
      "4  59.0  84.0   77.0  0      5     1       1      1\n",
      "5  43.0  84.0   77.0  0      6     1       1      1\n",
      "6  52.0  65.0   81.0  0      6     1       1      1\n",
      "ANTS ApplyTransformsToPoints:\n",
      "    x   y    z  t  label  mass  volume  count\n",
      "0 -51  -4   71  0      1     1       1      1\n",
      "1 -53 -43  108  0      2     2       1      1\n",
      "2 -52 -53   99  0      3     3       1      1\n",
      "3 -52 -65   96  0      4     4       1      1\n",
      "4 -59 -84   77  0      5     5       1      1\n",
      "5 -43 -84   77  0      6     6       1      1\n",
      "Selfmade ApplyTransformsToCSV:\n",
      "      x     y      z  t  label  mass  volume  count\n",
      "0  51.0   4.0   78.0  0      1     1       1      1\n",
      "1  53.0  43.0  152.0  0      2     1       1      1\n",
      "2  52.0  53.0  134.0  0      3     1       1      1\n",
      "3  52.0  65.0  128.0  0      4     1       1      1\n",
      "4  59.0  84.0   90.0  0      5     1       1      1\n",
      "5  43.0  84.0   90.0  0      6     1       1      1\n",
      "6  52.0  65.0   98.0  0      6     1       1      1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(\"/home/valentini/dev/Mousenet/dataset2/Feminad/\")\n",
    "affine_mris = sorted(glob(os.path.join(data_dir, 'MRI_N4_Resample_Norm_Identity_Affine', \"*12_6517*.nii.gz\")))\n",
    "#affine_csvs = sorted(glob(os.path.join(data_dir, 'Landmarks_Resample_Identity_Affine', \"*12_6517*apply.csv\")))\n",
    "affine_csvs_pos = sorted(glob(os.path.join(data_dir, 'Landmarks_Resample_Identity_Affine', \"*12_6517*apply_pos.csv\")))\n",
    "#affine_csvs_pos = sorted(glob(os.path.join(data_dir, 'Landmarks_Resample_Identity_Affine', \"*12_6517*Custom.csv\")))\n",
    "print(affine_csvs_pos)\n",
    "\n",
    "outfolder_custom = os.path.join(data_dir, 'Test')\n",
    "try:\n",
    "    os.mkdir(outfolder_custom)\n",
    "except Exception:\n",
    "     pass        \n",
    "    \n",
    "warp = Warp(\"bilinear\", \"border\")\n",
    "point = 50\n",
    "landmarks_template_antss = os.path.join(\"dataset2\", \"Atlas\", \"Identity_Feminad_Template_Landmarks_antss.csv\")\n",
    "landmarks_template_scipy = os.path.join(\"dataset2\", \"Atlas\", \"Identity_Feminad_Template_Landmarks_scipy.csv\")\n",
    "        \n",
    "for i in range(len(affine_mris)):\n",
    "    monaiwarp_mri_name = outfolder_custom + \"/MonaiWarp_\" + affine_mris[i].split('/')[-1]\n",
    "    mri = torch.from_numpy(nib.load(affine_mris[i]).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "    ddf_good = torch.from_numpy(custom_ddf.get_fdata()).permute(3,4,0,1,2)\n",
    "    monaiwarp_mri_data = warp(mri, ddf_good).squeeze()\n",
    "    monaiwarp_mri = nib.Nifti1Image(monaiwarp_mri_data, nib.load(ref).affine, nib.load(ref).header)  \n",
    "    nib.save(monaiwarp_mri, monaiwarp_mri_name)\n",
    "    \n",
    "\n",
    "    print('DDF: ' + str(ddf_good[0,:,point,point,point]))\n",
    "        \n",
    "    df_original = pd.read_csv(landmarks_template_scipy)\n",
    "    print('Original:')\n",
    "    print(df_original)\n",
    "    applypos_outname = outfolder_custom + \"/applypos_\" + affine_mris[i].split('/')[-1].split('.')[0] + '2.csv'\n",
    "    ants_command = \"antsApplyTransformsToPoints -d 3 -i \" + str(landmarks_template_antss) + \" -o \" + str(applypos_outname) + \" -t dataset2/Feminad/Custom_DDF.nii.gz\"\n",
    "    subprocess.call(ants_command.split(\" \"))\n",
    "    df_outants = pd.read_csv(applypos_outname)    \n",
    "    print('ANTS ApplyTransformsToPoints:')\n",
    "    print(df_outants)\n",
    "    \n",
    "    test = apply_warp_to_landmarks_df(landmarks_template_scipy, \"dataset2/Feminad/Custom_DDF.nii.gz\")\n",
    "    #test.to_csv(outfolder_custom + \"/test.csv\", index=False)    \n",
    "    print('Selfmade ApplyTransformsToCSV:')\n",
    "    print(test)    \n",
    " \n",
    "print('-'*40)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "9a61267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab254ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f48660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
