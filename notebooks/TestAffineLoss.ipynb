{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e839bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reg_mri\n",
    "import os\n",
    "from glob import glob\n",
    "from utils import compute_mean_dice\n",
    "import nibabel as nib\n",
    "from scipy.spatial.distance import dice\n",
    "import numpy as np\n",
    "import itk\n",
    "import SimpleITK as sitk\n",
    "import scipy.ndimage\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from transforms_dict import getRegistrationEvalInverseTransformForMRI, SaveTransformForMRI\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "import subprocess\n",
    "from monai.transforms import AsDiscrete, MaskIntensity, RandAffine, Affine\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9edd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.transforms import AsDiscrete, MaskIntensity, RandAffine, Affine\n",
    "from monai.utils import set_determinism\n",
    "from monai.losses import LocalNormalizedCrossCorrelationLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "\n",
    "import utils_parser\n",
    "from reg_data import getRegistrationDataset\n",
    "from reg_model import getRegistrationModel\n",
    "from utils import compute_mean_dice, getAdamOptimizer, getReducePlateauScheduler, loadExistingModel, getDevice\n",
    "from utils import print_model_output, print_weights, add_weights_to_name, compute_landmarks_distance_local\n",
    "from loss import compute_affine_loss, get_jacobian, antifolding_loss, JacobianDet\n",
    "from loss import get_deformable_registration_loss_from_weights, get_affine_registration_loss_from_weights, jacobian_loss\n",
    "from models import TrilinearLocalNet\n",
    "from torchinfo import summary\n",
    "from miseval import evaluate\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a58e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affine_loss_test(ddf1, ddf2, affine_matrix):\n",
    "    ## A(x+u'(x)) = x+u(x)\n",
    "    ## => Ax + Au'(x) = x+u(x)\n",
    "    ## => Ax-x + Au'(x)-u(x) = 0\n",
    "    ## => Affineloss = 1/128³*Sum(x€128³) |Ax+Au'(x) - (x+u(x))|²\n",
    "    # input = Ax+Au'(x) ||| target = x+u(x)\n",
    "    som = 0\n",
    "    device = getDevice()\n",
    "    #print(affine_matrix)\n",
    "    for i in range(128):\n",
    "        for j in range(128):\n",
    "            for k in range(128):\n",
    "                x = torch.tensor([[i, j, k]]).to(device)\n",
    "                \n",
    "                ux = ddf1[0,:,i,j,k]\n",
    "                ux = torch.tensor([[ux[0],ux[1],ux[2]]]).to(device)\n",
    "                ux = torch.transpose(ux,0,1)\n",
    "                ux = torch.mm(torch.eye(3).to(device), ux)\n",
    "                ux = torch.transpose(ux,0,1)\n",
    "    \n",
    "                x_aff = torch.tensor([[i,j,k,1]]).type(torch.cuda.FloatTensor).to(device)\n",
    "                x_aff = torch.transpose(x_aff,0,1)\n",
    "                Ax = torch.mm(affine_matrix,x_aff) \n",
    "                Ax = torch.transpose(Ax,0,1)\n",
    "                Ax = Ax[0,0:3]\n",
    "                \n",
    "                Aux = ddf2[0,:,i,j,k]\n",
    "                Aux_aff = torch.tensor([[Aux[0],Aux[1],Aux[2],1]]).type(torch.cuda.FloatTensor).to(device)\n",
    "                Aux_aff = torch.transpose(Aux_aff,0,1)\n",
    "                Aux = torch.mm(affine_matrix,Aux_aff)\n",
    "                Aux = torch.transpose(Aux,0,1)\n",
    "                Aux = Aux[0,0:3]                   \n",
    "                \n",
    "                input = Ax+Aux\n",
    "                target = x+ux\n",
    "                terme = torch.abs((input-target).squeeze())\n",
    "                term = torch.sum(terme)\n",
    "                som += term * term\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    som /= 128*128*128\n",
    "    return som                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25b27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affine_loss_vector(u1, u2, A):\n",
    "    ## => Affineloss = 1/128³*Sum(x€128³) |Ax+Au'(x) - (x+u(x))|²\n",
    "    \n",
    "    # input = AX+AU2(X) ||| target = X+U1(x)\n",
    "    \n",
    "    device = getDevice()    \n",
    "    #A = torch.linalg.inv(A)   \n",
    "    u1 = u1.to(device)\n",
    "    #print(u1.shape)\n",
    "    u2 = u2.to(device)    \n",
    "    \n",
    "    a = torch.arange(128,dtype=torch.float64, device=device)\n",
    "    one = torch.ones([128, 128, 128], dtype=torch.float64, device=device)\n",
    "    img = torch.stack([torch.meshgrid(a,a,a)[0], torch.meshgrid(a,a,a)[1], torch.meshgrid(a,a,a)[2]])\n",
    "    \n",
    "    img_aff = torch.stack([torch.meshgrid(a,a,a)[0], torch.meshgrid(a,a,a)[1], torch.meshgrid(a,a,a)[2], one]) \n",
    "    #print(img.shape)\n",
    "    #print(img[:,78,92,101])\n",
    "    \n",
    "    A_stack = A.type(torch.float64).repeat(128*128*128,1,1).to(device)\n",
    "\n",
    "    \n",
    "    X = img.view((3, 128*128*128)).permute(1,0).unsqueeze(2)    \n",
    "    #print(X.shape)\n",
    "    U1X = torch.as_tensor(u1.as_tensor().view((3, 128*128*128, 1)).permute(1,0,2), dtype=torch.float64, device=device)\n",
    "    #print(U1X.shape)\n",
    "    \n",
    "    X_aff = img_aff.view((4, 128*128*128)).permute(1,0).unsqueeze(2)    \n",
    "    U2X = torch.as_tensor(u2.as_tensor().view((3, 128*128*128, 1)).permute(1,0,2), dtype=torch.float64, device=device)   \n",
    "    one_aff = torch.ones([128*128*128,1], dtype=torch.float64, device=device)\n",
    "    U2X_aff = torch.stack([U2X[:,0], U2X[:,1], U2X[:,2], one_aff], dim=1)\n",
    "    AXplusAU2X = torch.bmm(A_stack, X_aff+U2X_aff)[:,0:3]#.squeeze().permute(1,0).view((4,128,128,128))\n",
    "    #print(AXplusAU2X.shape)\n",
    "    \n",
    "    input = AXplusAU2X\n",
    "    target = X+U1X\n",
    "    loss = torch.nn.MSELoss()\n",
    "    affine_loss = loss(input, target)\n",
    "    \n",
    "    return affine_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c3f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affine_loss_brouillon(u1, u2, A):\n",
    "    ## => Affineloss = 1/128³*Sum(x€128³) |Ax+Au'(x) - (x+u(x))|²\n",
    "    \n",
    "    # input = AX+AU2(X) ||| target = X+U1(x)\n",
    "    \n",
    "    device = getDevice()    \n",
    "    A = torch.linalg.inv(A).type(torch.FloatTensor)\n",
    "    print(A)\n",
    "    u1 = u1.to(device).squeeze().reshape(3,128*128*128).type(torch.FloatTensor)\n",
    "    u2 = u2.to(device).squeeze().reshape(3,128*128*128).type(torch.FloatTensor)\n",
    "    \n",
    "    a = torch.arange(128,dtype=torch.float64, device=device)    \n",
    "    x = torch.stack(torch.meshgrid(a,a,a)).view(3,128*128*128)\n",
    "    \n",
    "    h1 = x + u1\n",
    "    \n",
    "    h2 = x + u2\n",
    "    h2 = h2.view((3,128*128*128)).type(torch.FloatTensor)\n",
    "    \n",
    "    h2 = torch.mm(A[:3,:3],h2) + A[:3,3].unsqueeze(1)\n",
    "    \n",
    "    loss = torch.nn.MSELoss()\n",
    "    affine_loss = loss(h1, h2)\n",
    "    return affine_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640edc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1., 0., 0., 2.],\n",
    "                    [0., 1., 0., 3.],\n",
    "                    [0., 0., 1., 4.],\n",
    "                    [0., 0., 0., 1.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5fcde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "T = A[:3,3].reshape(1,3,1,1,1)\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d6ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.tensor([[1., 0., 0., 0.],\n",
    "                    [0., 0., -1., 0.],\n",
    "                    [0., 1., 0., 0.],\n",
    "                    [0., 0., 0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a86556e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mddf\u001B[49m\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ddf' is not defined"
     ]
    }
   ],
   "source": [
    "print(ddf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8ea017",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m ddf2 \u001B[38;5;241m=\u001B[39m \u001B[43mddf\u001B[49m\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ddf' is not defined"
     ]
    }
   ],
   "source": [
    "ddf2 = ddf.permute(0,1,2,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "39a51d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06156899,  0.71789205, -0.0716481 ], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf[0,:,78,101,123].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0f788016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06156899,  0.71789205, -0.0716481 ], dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf2[0,:,78,123,101].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc76e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_affine_loss_brouillon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcompute_affine_loss_brouillon\u001B[49m(ddf, ddf2, T)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'compute_affine_loss_brouillon' is not defined"
     ]
    }
   ],
   "source": [
    "compute_affine_loss_brouillon(ddf, ddf2, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9d170ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5402.0856, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_affine_loss_vector(ddf, ddf, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c8473e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.6532774"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a2260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "731207d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper-old-0.1: aff: 38.1056\n",
      "paper-affine-0.1: aff: 33.3974\n",
      "paper-old-8.0: aff: 37.8664\n",
      "paper-affine-8.0: aff: 33.1848\n"
     ]
    }
   ],
   "source": [
    "outfolders = [\n",
    "        \"paper-old-0.1\",\n",
    "        \"paper-affine-0.1\",   \n",
    "        \"paper-old-8.0\",\n",
    "        \"paper-affine-8.0\",   \n",
    "]\n",
    "models = [\n",
    "    \"paper/local_overfit_feminad_old_1.0-0.0-0.1.pth\",\n",
    "    \"paper/local_overfit_feminad_old_affine_1.0-0.0-0.1.pth\",\n",
    "    \"paper/local_overfit_feminad_old_1.0-0.0-8.0.pth\",\n",
    "    \"paper/local_overfit_feminad_old_affine_1.0-0.0-8.0.pth\",\n",
    "]\n",
    "\n",
    "outdataset = 'Feminad'\n",
    "mris = sorted(glob(os.path.join('dataset2', outdataset, 'MRI_N4_Resample_Norm_Identity_Affine', \"*.nii.gz\")))\n",
    "atlas_name = \"dataset2/Atlas/Identity_Feminad_Template.nii.gz\"\n",
    "affine = nib.load(atlas_name).affine\n",
    "header = nib.load(atlas_name).header \n",
    "\n",
    "sums = [0,0,0,0]\n",
    "for mri in mris:\n",
    "    nib_mri = nib.load(mri).get_fdata()\n",
    "    nib_mri = np.expand_dims(nib_mri, axis=0)    \n",
    "    randaffine_transform = RandAffine(\n",
    "                            mode='bilinear',\n",
    "                            prob=1.0,\n",
    "                            rotate_range=(np.pi/45, np.pi/45, np.pi/45),\n",
    "                            scale_range=(0.1, 0.1, 0.1),\n",
    "                            translate_range=(2, 2, 2),\n",
    "                        )\n",
    "    randaffine_moving_image = randaffine_transform(nib_mri).unsqueeze(0)\n",
    "    randaffine_matrix = randaffine_transform.rand_affine_grid.get_transformation_matrix()     \n",
    "    nib.save(nib.Nifti1Image(randaffine_moving_image.squeeze(), affine, header), \"tmp3.nii.gz\")\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        if \"newmodel\" in model:\n",
    "            newmodel = True\n",
    "        else:\n",
    "            newmodel = False\n",
    "        pred_image, ddfs = reg_mri.main(model, mri, \"tmp1\", False, False, \"local\", newmodel=newmodel)\n",
    "        u1 = ddfs[1]        \n",
    "        pred_image, ddfs = reg_mri.main(model, \"tmp3.nii.gz\", \"tmp2\", False, False, \"local\", newmodel=newmodel)\n",
    "        u2 = ddfs[1]\n",
    "        affine_loss = compute_affine_loss_vector(u1,u2,randaffine_matrix)\n",
    "        sums[i] += affine_loss\n",
    "for i in range(len(sums)):\n",
    "    sums[i] /= len(mris)\n",
    "    outmessage = \"{}: aff: {:.4f}\".format(outfolders[i], sums[i])\n",
    "    print(outmessage)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894d4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989990a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6866a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ed833d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving to testptdr_1.0-0.0-8.0.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 37>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     35\u001B[0m     channels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m\n\u001B[1;32m     36\u001B[0m     extract \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m---> 37\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mgetRegistrationModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mregistration_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrain_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mchannels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextract\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_ddf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_ddf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m getAdamOptimizer(model, lr)\n\u001B[1;32m     41\u001B[0m learningrate \u001B[38;5;241m=\u001B[39m lr\n",
      "File \u001B[0;32m~/dev/Mousenet/reg_model.py:119\u001B[0m, in \u001B[0;36mgetRegistrationModel\u001B[0;34m(registration_type, img_size, pretrain_model, channels, extract, use_ddf)\u001B[0m\n\u001B[1;32m    117\u001B[0m     model \u001B[38;5;241m=\u001B[39m GlobalNet(img_size\u001B[38;5;241m=\u001B[39mimg_size, use_dvf\u001B[38;5;241m=\u001B[39muse_dvf)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m registration_type\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 119\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mLocalNet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchannels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextract\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_dvf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_dvf\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m registration_type\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdeformable\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    121\u001B[0m     model \u001B[38;5;241m=\u001B[39m DeformableNet(img_size, pretrain_model\u001B[38;5;241m=\u001B[39mpretrain_model, channels\u001B[38;5;241m=\u001B[39mchannels, extract\u001B[38;5;241m=\u001B[39mextract, use_dvf\u001B[38;5;241m=\u001B[39muse_dvf)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/dev/Mousenet/reg_model.py:59\u001B[0m, in \u001B[0;36mLocalNet.__init__\u001B[0;34m(self, channels, extract, use_dvf)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28msuper\u001B[39m(LocalNet, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     50\u001B[0m     spatial_dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m     51\u001B[0m     in_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     56\u001B[0m     out_kernel_initializer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkaiming_uniform\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m )\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m getDevice()\n\u001B[0;32m---> 59\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarp \u001B[38;5;241m=\u001B[39m Warp(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mborder\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarp_nearest \u001B[38;5;241m=\u001B[39m Warp(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnearest\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mborder\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/dev/Mousenet/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:907\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    903\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    904\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    905\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 907\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/Mousenet/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 578\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    582\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    583\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    589\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/Mousenet/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 578\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    582\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    583\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    589\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 578 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/dev/Mousenet/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 578\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    582\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    583\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    589\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/Mousenet/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:601\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 601\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    602\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/dev/Mousenet/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    904\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m--> 905\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "modelname = \"testptdr.pth\"\n",
    "dataset = \"feminadaffine\"\n",
    "ft = None\n",
    "ct = None\n",
    "batchsize = 1 \n",
    "max_epochs = 1000\n",
    "lr = 0.001\n",
    "patience = 20\n",
    "weights = [1.0, 0.0, 8.0]\n",
    "registration_type = \"local\"\n",
    "atlas = True\n",
    "mask = False\n",
    "pt = None\n",
    "newmodel = False \n",
    "validfeminad = True\n",
    "freeze = 0\n",
    "cycle_consistent_training = False \n",
    "use_jacobian_loss = False\n",
    "affine_consistent_training = True\n",
    "use_antifolding_loss = False\n",
    "use_ddf = False    \n",
    "    \n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "modelname = add_weights_to_name(modelname, weights)\n",
    "print_model_output(modelname)\n",
    "set_determinism(seed=0)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "device = getDevice()\n",
    "\n",
    "if newmodel:\n",
    "    channels = 32\n",
    "    extract = [0, 1, 2, 3, 4]\n",
    "else:\n",
    "    channels = 16\n",
    "    extract = [0, 1, 2, 3]\n",
    "model = getRegistrationModel(registration_type, img_size=128, pretrain_model=pt,\n",
    "                                 channels=channels, extract=extract, use_ddf=use_ddf)\n",
    "\n",
    "optimizer = getAdamOptimizer(model, lr)\n",
    "learningrate = lr\n",
    "scheduler = getReducePlateauScheduler(optimizer, factor=0.5, patience=20)\n",
    "weights = loadExistingModel(model, optimizer, ft, ct, weights=weights, registration=True)\n",
    "print_weights(weights)\n",
    "\n",
    "dataloaders, size = getRegistrationDataset(dataset=dataset,\n",
    "                                           batch=batchsize,\n",
    "                                           training=True,\n",
    "                                           augment=True,\n",
    "                                           eval_augment=False,\n",
    "                                           atlas=atlas,\n",
    "                                           mask=mask,\n",
    "                                           validfeminad=validfeminad,\n",
    "                                           )\n",
    "\n",
    "best_loss = np.inf\n",
    "best_epoch = -1\n",
    "\n",
    "sizelol = len(dataloaders[\"train\"])\n",
    "print(sizelol)\n",
    "\n",
    "#for epoch in range(-1, max_epochs):\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "\n",
    "    train_loss, train_metric, train_dice, train_lbl_loss, train_img_loss, train_ddf_loss = 0, 0, 0, 0, 0, 0\n",
    "    valid_loss, valid_metric, valid_dice, valid_lbl_loss, valid_img_loss, valid_ddf_loss = 0, 0, 0, 0, 0, 0\n",
    "    train_aff_loss, valid_aff_loss = 0, 0\n",
    "\n",
    "    for phase in ['train', 'valid']:\n",
    "        if epoch == -1 and phase == 'train':\n",
    "            continue\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        elif phase == 'valid':\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        running_aff_loss = 0.0\n",
    "\n",
    "        running_metric = 0.0\n",
    "        running_dice = 0.0\n",
    "        \n",
    "        running_img_loss = 0.0\n",
    "        running_lbl_loss = 0.0\n",
    "        running_ddf_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(dataloaders[phase]):\n",
    "            if i >= sizelol:\n",
    "                break\n",
    "\n",
    "            print(i, end='\\r')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                ddf, pred_image, pred_label, dvf = model(data)\n",
    "\n",
    "                pred_image = pred_image.to(device, non_blocking=True)\n",
    "                pred_label = pred_label.to(device, non_blocking=True)\n",
    "                pred_mask = AsDiscrete(threshold=0.5)(pred_label)\n",
    "                pred_image_masked = MaskIntensity(mask_data=pred_mask)(pred_image)\n",
    "\n",
    "                fixed_image = data['fixed_image'].to(device, non_blocking=True)\n",
    "                fixed_label = data['fixed_label'].to(device, non_blocking=True)\n",
    "                fixed_mask = AsDiscrete(threshold=0.5)(fixed_label)\n",
    "                fixed_image_masked = MaskIntensity(mask_data=fixed_mask)(fixed_image)\n",
    "\n",
    "                img_loss, lbl_loss, ddf_loss = get_deformable_registration_loss_from_weights(pred_image_masked,\n",
    "                                                                                                     pred_mask,\n",
    "                                                                                                     fixed_image_masked,\n",
    "                                                                                                     fixed_mask,\n",
    "                                                                                                     dvf,\n",
    "                                                                                                     weights)\n",
    "                loss = img_loss + lbl_loss + ddf_loss\n",
    "                \n",
    "                if affine_consistent_training:\n",
    "                    randaffine_transform = RandAffine(\n",
    "                        mode='bilinear',\n",
    "                        prob=1.0,\n",
    "                        rotate_range=(np.pi/90, np.pi/90, np.pi/90),\n",
    "                        scale_range=(0.05, 0.05, 0.05),\n",
    "                        translate_range=(2, 2, 2),\n",
    "                    )\n",
    "                    randaffine_moving_image = randaffine_transform(data[\"moving_image\"][0, :, :, :, :]).unsqueeze(0)\n",
    "                    randaffine_matrix = randaffine_transform.rand_affine_grid.get_transformation_matrix().to(device)\n",
    "                    randaffine_transform_nearest = Affine(\n",
    "                        mode='nearest',\n",
    "                        affine=randaffine_matrix\n",
    "                    )\n",
    "                    randaffine_moving_label, _ = randaffine_transform_nearest(data[\"moving_label\"][0, :, :, :, :])\n",
    "                    randaffine_moving_label = randaffine_moving_label.unsqueeze(0)\n",
    "                    randaffine_data = {\n",
    "                        \"fixed_image\": fixed_image,\n",
    "                        \"fixed_label\": fixed_label,\n",
    "                        \"moving_image\": randaffine_moving_image,\n",
    "                        \"moving_label\": randaffine_moving_label,\n",
    "                    }\n",
    "\n",
    "                    if registration_type.lower() == 'local':\n",
    "                        randaffine_ddf, randaffine_pred_image, randaffine_pred_label, randaffine_dvf = model(randaffine_data)\n",
    "                        randaffine_pred_image = randaffine_pred_image.to(device, non_blocking=True)\n",
    "                        randaffine_pred_label = randaffine_pred_label.to(device, non_blocking=True)\n",
    "                        randaffine_pred_mask = AsDiscrete(threshold=0.5)(randaffine_pred_label)\n",
    "                        randaffine_pred_image_masked = MaskIntensity(mask_data=randaffine_pred_mask)(randaffine_pred_image)\n",
    "\n",
    "                        affine_img_loss, affine_lbl_loss, affine_ddf_loss = get_deformable_registration_loss_from_weights(\n",
    "                                randaffine_pred_image_masked,\n",
    "                                randaffine_pred_mask,\n",
    "                                fixed_image_masked,\n",
    "                                fixed_mask,\n",
    "                                randaffine_dvf,\n",
    "                                weights)\n",
    "\n",
    "                        loss2 = affine_img_loss + affine_lbl_loss + affine_ddf_loss\n",
    "\n",
    "                        affine_loss = compute_affine_loss_vector(ddf, randaffine_ddf, randaffine_matrix)\n",
    "                        loss = loss + loss2 + affine_loss\n",
    "\n",
    "\n",
    "                dice_metric = compute_mean_dice(pred_mask, fixed_mask)\n",
    "                metric = np.mean(compute_landmarks_distance_local(ddf, data)[1:])\n",
    "\n",
    "                if phase == 'train':\n",
    "                    learningrate = optimizer.param_groups[0]['lr']\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * fixed_image.size(0)\n",
    "            running_aff_loss += affine_loss.item() * fixed_image.size(0)\n",
    "            running_metric += metric.item() * fixed_image.size(0)\n",
    "            running_dice += dice_metric.item() * fixed_image.size(0)\n",
    "            running_img_loss += img_loss.item() * fixed_image.size(0)\n",
    "            running_lbl_loss += lbl_loss.item() * fixed_image.size(0)\n",
    "            running_ddf_loss += ddf_loss.item() * fixed_image.size(0)\n",
    "\n",
    "\n",
    "        running_loss /= sizelol\n",
    "        running_aff_loss /= sizelol\n",
    "        running_metric /= sizelol\n",
    "        running_dice /= sizelol\n",
    "        running_img_loss /= sizelol\n",
    "        running_lbl_loss /= sizelol\n",
    "        running_ddf_loss /= sizelol\n",
    "\n",
    "        if phase == 'train':\n",
    "            train_loss, train_metric, train_img_loss, train_lbl_loss, train_ddf_loss = (\n",
    "                running_loss, running_metric, running_img_loss, running_lbl_loss, running_ddf_loss)\n",
    "            train_dice = running_dice\n",
    "            train_aff_loss = running_aff_loss\n",
    "        elif phase == 'valid':\n",
    "            valid_loss, valid_metric, valid_img_loss, valid_lbl_loss, valid_ddf_loss = (\n",
    "                running_loss, running_metric, running_img_loss, running_lbl_loss, running_ddf_loss)\n",
    "            valid_dice = running_dice\n",
    "            valid_aff_loss = running_aff_loss\n",
    "\n",
    "        outmessage = \"{}: loss: {:.4f} - metric: {:.4f} -- img: {:.4f}, lbl: {:.4f}, ddf: {:.4f}\".format(\n",
    "                phase, running_loss, running_metric, running_img_loss, running_lbl_loss, running_ddf_loss)\n",
    "        outmessage += \" -- aff: {:.4f}\".format(running_aff_loss)\n",
    "        outmessage += \" -- dice: {:.4f}\".format(running_dice)\n",
    "        print(outmessage)\n",
    "\n",
    "        if (phase == 'valid' and not validfeminad) or (phase == 'train' and validfeminad):\n",
    "            scheduler.step(running_loss)\n",
    "            if running_loss < best_loss:\n",
    "                best_loss = running_loss\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'weights': weights,\n",
    "                    'epoch': epoch,\n",
    "                    'lr': lr,\n",
    "                    },\n",
    "                    './models/' + modelname\n",
    "                )\n",
    "                print(\n",
    "                    \"best loss {:.4f} at epoch {}\".format(\n",
    "                        best_loss, best_epoch\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ed1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
